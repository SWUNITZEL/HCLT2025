# íŒŒì¼ëª…: multi_model_evaluator.py

import os
import json
import numpy as np
import time
from typing import List, Dict, Any

# --- ì‚¬ì „ ì¤€ë¹„ ---
from utils.gpt_api_utils import (
    call_gpt4_with_model, 
    call_gpt5_with_model, 
    call_claude_with_model, 
    call_gemini_with_model
)
from rouge_score import rouge_scorer

class MultiModelEvaluator:
    """
    ë¯¸ë¦¬ í†µí•©ëœ 300ê°œì˜ QA ì„¸íŠ¸ íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬,
    student_agent.txt í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ìœ¼ë¡œ ì—¬ëŸ¬ LLM ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” íŒŒì´í”„ë¼ì¸.
    """
    def __init__(self):
        self.eval_dir = "data/qa/unified_eval_results_300"
        self.unified_data_path = os.path.join(self.eval_dir, "unified_300_qa_sets.json")
        os.makedirs(self.eval_dir, exist_ok=True)

        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=False)

        # âœ… 1. student_agent.txtë¥¼ ë¡œë“œí•˜ê³  system/user í…œí”Œë¦¿ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
        self.system_template, self.user_template = self._load_and_split_prompt_template()

        # --- âœ… í‰ê°€í•  ëª¨ë¸ê³¼ ì„¤ì • ì •ì˜ ---
        self.models_to_evaluate = {
            "GPT4o_on": { "func": call_gpt4_with_model, "model_name": "gpt-4o", "reasoning": True },
            "GPT4o_off": { "func": call_gpt4_with_model, "model_name": "gpt-4o-mini", "reasoning": False },
        }

    # âœ… 2. student_agent.txtë¥¼ ë¡œë“œí•˜ê³  ë¶„ë¦¬í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
    def _load_and_split_prompt_template(self) -> (str, str):
        prompt_path = os.path.join("config", "prompts", "student_agent.txt")
        if not os.path.exists(prompt_path):
            raise FileNotFoundError(f"í”„ë¡¬í”„íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {prompt_path}")
        
        with open(prompt_path, "r", encoding="utf-8") as f:
            full_template = f.read()

        if '---' not in full_template:
            raise ValueError("í”„ë¡¬í”„íŠ¸ íŒŒì¼ì— systemê³¼ user í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ë¶„í•˜ëŠ” '---' ë§ˆì»¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        system_part, user_part = full_template.split('---', 1)
        return system_part.strip(), user_part.strip()

    def load_unified_data(self) -> List[Dict[str, Any]]:
        """í†µí•©ëœ QA ì„¸íŠ¸ JSON íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        print(f"í†µí•© ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘: '{self.unified_data_path}'")
        if not os.path.exists(self.unified_data_path):
            raise FileNotFoundError(f"ì˜¤ë¥˜: í†µí•© ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € prepare_dataset.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        with open(self.unified_data_path, "r", encoding="utf-8") as f:
            return json.load(f)

    # âœ… 3. dispatch_api_call í•¨ìˆ˜ê°€ ë¶„ë¦¬ëœ í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
    def dispatch_api_call(self, model_info: Dict[str, Any], qa_item: Dict[str, Any]) -> str:
        """ì„¤ì •ì— ë§ëŠ” ëª¨ë¸ì˜ APIë¥¼ í˜¸ì¶œí•˜ê³  ì‘ë‹µì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
        api_func = model_info["func"]
        model_name = model_info["model_name"]
        reasoning = model_info["reasoning"]
        
        # qa_itemì—ì„œ í•„ìš”í•œ ëª¨ë“  ì •ë³´ ì¶”ì¶œ
        question = qa_item["question"]
        department = qa_item.get("department", "í•´ë‹¹ í•™ê³¼")
        document = qa_item.get("document", "ì œê³µëœ ë¬¸ì„œ ì—†ìŒ")
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì±„ìš°ê¸°
        system_prompt = self.system_template.format(department=department)

        # ìœ ì € í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì±„ìš°ê¸°
        # ë‹¨ì¼ ì§ˆë¬¸ì„ {questions} í”Œë ˆì´ìŠ¤í™€ë” í˜•ì‹ì— ë§ê²Œ ë³€í™˜
        questions_text = f"1. {question}"
        user_prompt = self.user_template.format(questions=questions_text, document=document)
        
        print(f"    ëª¨ë¸ í˜¸ì¶œ: {model_name} (reasoning={'on' if reasoning else 'off'})")
        print(f"    ì§ˆë¬¸: {question[:100]}...")
        
        try:
            answer = api_func(system_prompt, user_prompt, model_name, reasoning=reasoning)
            
            refusal_keywords = ["ì£„ì†¡", "í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ìš”ì²­ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "I cannot fulfill", "I'm unable to"]
            if any(keyword in answer for keyword in refusal_keywords):
                print(f"    âš ï¸ ê²½ê³ : ëª¨ë¸ì´ ë‹µë³€ì„ ê±°ë¶€í–ˆìŠµë‹ˆë‹¤. ì‘ë‹µ: {answer[:150]}...")
            else:
                print(f"    --> ëª¨ë¸ ì‘ë‹µ: {answer[:200]}...")
            
            return answer
        except Exception as e:
            print(f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({model_name}, reasoning={'on' if reasoning else 'off'}): {e}")
            return "[API ERROR]"

    def calculate_max_rouge_score(self, generated_answer: str, ground_truths: List[str]) -> Dict[str, float]:
        """í•˜ë‚˜ì˜ ìƒì„±ëœ ë‹µë³€ê³¼ ì—¬ëŸ¬ ì •ë‹µ í›„ë³´ ê°„ì˜ ROUGE ì ìˆ˜ ì¤‘ ê°€ì¥ ë†’ì€ F1 ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not generated_answer or not ground_truths:
            return {"rouge1_f1": 0.0, "rougeL_f1": 0.0}

        scores = [self.rouge_scorer.score(gt, generated_answer) for gt in ground_truths]
        max_rouge1_f1 = max([s['rouge1'].fmeasure for s in scores]) if scores else 0.0
        max_rougeL_f1 = max([s['rougeL'].fmeasure for s in scores]) if scores else 0.0
        
        return {"rouge1_f1": max_rouge1_f1, "rougeL_f1": max_rougeL_f1}

    # âœ… 4. run_full_evaluation í•¨ìˆ˜ëŠ” qa_item ì „ì²´ë¥¼ ë„˜ê²¨ì£¼ë¯€ë¡œ ìˆ˜ì •í•  í•„ìš” ì—†ìŒ
    def run_full_evaluation(self):
        """ì •ì˜ëœ ëª¨ë“  ëª¨ë¸ê³¼ ì„¤ì •ì— ëŒ€í•´ ì „ì²´ í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        all_qa_sets = self.load_unified_data()
        total_sets = len(all_qa_sets)
        overall_summary = {}

        for run_key, model_info in self.models_to_evaluate.items():
            print(f"\n{'='*20}\nğŸš€ '{run_key}' í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤... ({total_sets}ê°œ ì§ˆë¬¸)\n{'='*20}")

            run_results = []
            all_rouge1_f1 = []
            all_rougeL_f1 = []

            for i, qa_item in enumerate(all_qa_sets):
                print(f"  -> {run_key}: ì§ˆë¬¸ {i+1}/{total_sets} ì²˜ë¦¬ ì¤‘...")
                
                generated_answer = self.dispatch_api_call(model_info, qa_item)
                scores = self.calculate_max_rouge_score(generated_answer, qa_item["ground_truths"])
                
                run_results.append({
                    "unified_id": qa_item["unified_id"],
                    "question": qa_item["question"],
                    "generated_answer": generated_answer,
                    "scores": scores
                })
                all_rouge1_f1.append(scores["rouge1_f1"])
                all_rougeL_f1.append(scores["rougeL_f1"])
                
                time.sleep(1)

            avg_rouge1_f1 = np.mean(all_rouge1_f1)
            avg_rougeL_f1 = np.mean(all_rougeL_f1)

            overall_summary[run_key] = {
                "ROUGE-1_F1_avg": avg_rouge1_f1,
                "ROUGE-L_F1_avg": avg_rougeL_f1,
            }

            detailed_filename = os.path.join(self.eval_dir, f"detailed_results_{run_key}.json")
            with open(detailed_filename, "w", encoding="utf-8") as f:
                json.dump(run_results, f, ensure_ascii=False, indent=2)
            
            print(f"'{run_key}' í‰ê°€ ì™„ë£Œ! í‰ê·  ROUGE-1 F1: {avg_rouge1_f1:.4f}, ROUGE-L F1: {avg_rougeL_f1:.4f}")

        summary_filename = os.path.join(self.eval_dir, "evaluation_summary_all_models.json")
        with open(summary_filename, "w", encoding="utf-8") as f:
            json.dump(overall_summary, f, ensure_ascii=False, indent=2)

        print("\n\n--- ğŸ† ìµœì¢… í‰ê°€ ìš”ì•½ ğŸ† ---")
        for run_name, scores in overall_summary.items():
            print(f"  - {run_name}:")
            print(f"    - ROUGE-1 F1 í‰ê· : {scores['ROUGE-1_F1_avg']:.4f}")
            print(f"    - ROUGE-L F1 í‰ê· : {scores['ROUGE-L_F1_avg']:.4f}")
        print(f"\nëª¨ë“  í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìƒì„¸ ê²°ê³¼ëŠ” '{self.eval_dir}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    evaluator = MultiModelEvaluator()
    evaluator.run_full_evaluation()